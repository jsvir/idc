{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJAeukhWPoCarXSwh/Jeax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsvir/idc/blob/main/idc_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretable Deep Clustering (for Tabular Data)\n",
        "\n",
        "You can use **any** data, also images, but for now the model supports only samples similar to table rows: each sample should be a d-dimensional vector.\n",
        "The main goal of our method is to discover clusters assignments for the dataset samples and provide **local** (sample-level) and **global** (cluster-level) interpretations. The interpretations are the feature ids that are have the most important information for clustering and are potentially not representing the data noise.\n",
        "\n",
        "## Model Description:\n",
        "\n",
        "<img src=\"https://github.com/jsvir/idc/tree/main/img/img.png\" width=\"500\">\n",
        "\n",
        "We train a Gating Neural Network together with autoencoder with reconstruction objective while our goal to reconstruct the sample x from the gated version of it (x * z). Then we train the clustering head to discover the clustering of the samples. The last stage is to train the auxiliary classifier that trains the global gates matrix for cluster-level features (interpretations). In addition, we add more sub-steps for training that serve as augmentations to the main stages. We add random binary noise to the input samples, we add noise to the latent embeddings (after encoder) and we start train the autoencoder without gating network.\n",
        "\n",
        "Next, we will go step-by-step with MNIST example to show how the training is done. If you find something unclear, please, let us know."
      ],
      "metadata": {
        "id": "2GNTXcVzYsuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: config file definitions\n",
        "\n",
        "| Key                                  | Required / Optional | Example Value                  | Description                                                                                                                                                                                                                                                 |\n",
        "|--------------------------------------|---------------------|--------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| dataset                              | Required            | MNIST10K                       | *The dataset class name in dataset.py. The dataset class should implement **setup** function for preprocessing.*                                                                                                                                            |\n",
        "| data_dir                             | Optional            | C:/data/fs/mnist               | *Dataset directory path. Depends if your **setup** function needs it*                                                                                                                                                                                       |\n",
        "| scaler                               | Optional            | MinMaxScaler                   | *Depends if it is required by your dataset **setup** function*                                                                                                                                                                                              |\n",
        "| batch_size                           | Required            | 100                            | *Training and evaluation batch size*                                                                                                                                                                                                                        |\n",
        "| epochs                               | Required            | 100                            | *How many epochs to train the model (total epochs)*                                                                                                                                                                                                         |\n",
        "| seeds                                | Required            | 1                              | *How many random intializations for model re-training*                                                                                                                                                                                                      |\n",
        "| ae_non_gated_epochs                  | Required            | 10                             | *Number of epochs for autoencoder pre-training without gating network.*                                                                                                                                                                                     |\n",
        "| ae_pretrain_epochs                   | Required            | 20                             | *Number of epochs for autoencoder pre-training with gating network.*                                                                                                                                                                                        |\n",
        "| start_global_gates_training_on_epoch | Required            | 50                             | *After this number of epochs we start to train aux. classifier with global gates.*                                                                                                                                                                          |\n",
        "| mask_percentage                      | Required            | 0.9                            | *The random subset of features that will be masked by zero gates. The tuning of this parameter should be based on reconstruction loss convergence. For better convergence try smaller values. Far sparse mask try larger*                                   |\n",
        "| latent_noise_std                     | Required            | 0.01                           | *The std value for random normal noise with mean=1 that multiplies latent embeddings (H) outputed by the encoder.*                                                                                                                                          |\n",
        "| gtcr_loss                            | Optional            | true                           | *Use it to encourge features uniquness at sample-level (the model will try to find the unique set of features for each sample.*                                                                                                                             |\n",
        "| gtcr_projection_dim                  | Optional            | null                           | *For large number of features (>10K) it will apply a random projection to the smaller dimension which affects only the GTCR loss*                                                                                                                           |\n",
        "| gtcr_eps                             | Optional            | 1                              | *Code Reduction Rate precision parameter *                                                                                                                                                                                                                  |\n",
        "| eps                                  | Required            | 0.1                            | *Clustering head loss is trained with code reduction rate -based objective with precision parameter. Notice, that here the loss operates on latent embedding and helps to cluster them while gtcr operates on gates only and try to seperate between them.* |\n",
        "| use_gating                           | Required            | true                           | *If trained with Gating Network.*                                                                                                                                                                                                                           |\n",
        "| gates_hidden_dim                     | Required            | 784                            | *The hodden layer dimension in Gating Network.*                                                                                                                                                                                                             |\n",
        "| encdec                               | Required            | [512,512,2048,10,2048,512,512] | *Autoencoder architecture. Each value represents the hidden layer dimension*                                                                                                                                                                                |\n",
        "| clustering_head                      | Required            | [10, 2048]                     | *Clustering head dimension. The input dimension and the hidden dimension.*                                                                                                                                                                                  |\n",
        "| tau                                  | Required            | 100                            | *Tempretaure for GumbleSoftmax. We used a fixed value but you can try also to change it dring the training*                                                                                                                                                 |\n",
        "| aux_classifier                       | Required            | 2048                           | *TThe dimension of the hidden layer in the aux classifier*                                                                                                                                                                                                  |\n",
        "| local_gates_lambda                   | Required            | 1                              | *The weight of the sparsity loss term in the total clustering loss computation.*                                                                                                                                                                            |\n",
        "| global_gates_lambda                  | Required            | 1                              | *The weight of the sparsity loss term in the total aix classifier loss computation.*                                                                                                                                                                        |\n",
        "| gtcr_lambda                          | Required            | 0.01                           | *The weight of the uniqness loss term in the total clustering loss computation.*                                                                                                                                                                            |\n",
        "| lr.pretrain                          | Required            | 1e-3                           | *The learning rate for the autoencoder and gating networks.*                                                                                                                                                                                                |\n",
        "| lr.clustering                        | Required            | 1e-2                           | *The learning rate for the clustering head.*                                                                                                                                                                                                                |\n",
        "| lr.aux_classifier                    | Required            | 1e-2                           | *The learning rate for the aux classifier and global gates matrix.*                                                                                                                                                                                         |\n",
        "| sched.pretrain_min_lr                | Required            | 1e-6                           | *The min learning rate for the autoencoder and gating networks.*                                                                                                                                                                                            |\n",
        "| sched.clustering_min_lr              | Required            | 1e-6                           | *The min learning rate for the clustering head.*                                                                                                                                                                                                            |\n",
        "| save_seed_checkpoints                | Required            | false                          | *Change it to true if you would like to save the checkpoint.*                                                                                                                                                                                               |\n",
        "| validate                             | Optional               | true                           | *If you have a labeled data and wish to check the method on it (like MNIST) then use true. Otherwise false*                                                                                                                                                 |\n",
        "\n",
        "And finally there are some additional pytorch-lightning configs you should provide but it could remain the same valeus as below:\n",
        "\n",
        "trainer:\n",
        "  devices: 1\n",
        "  accelerator: gpu\n",
        "  max_epochs: *epochs\n",
        "  deterministic: true\n",
        "  logger: true\n",
        "  log_every_n_steps: 10\n",
        "  check_val_every_n_epoch: 10\n",
        "  enable_checkpointing: false\n",
        "  \n",
        "\n",
        "We clone the repo and print the yaml config file we will use for MNIST."
      ],
      "metadata": {
        "id": "83eDT3CZYxxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jsvir/idc.git && cd idc && pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aWVSGJRY6CC",
        "outputId": "f82d0bfb-cdd2-4ea9-a79e-64c9767ac10f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'idc' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls idc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux5fU_4pliF1",
        "outputId": "3f455cf9-a9af-4f68-886f-44f39d038ca9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"idc/cfg/cfg_mnist.yaml\") as f:\n",
        "    for line in f.readlines():\n",
        "        print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "xYoZhc_lY9tU",
        "outputId": "1b348ec0-def2-4d07-f1bc-aee315ce8c4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'idc/cfg/cfg_mnist.yaml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-544260e90ffc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"idc/cfg/cfg_mnist.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'idc/cfg/cfg_mnist.yaml'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: add your dataset class\n",
        "\n",
        "Assuming you have a dataset numpy files that are ready for training, this is the minimal code you need (*The X values should pass z-score prerpocessing. For some cases like MNIST dataset MinMax(0,1) could be also applied*):"
      ],
      "metadata": {
        "id": "LTf07gJRZAqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import ClusteringDataset\n",
        "from sklearn import preprocessing\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "\n",
        "class MNIST10K(ClusteringDataset):\n",
        "    def __init__(self, data, targets):\n",
        "        super().__init__(data, targets)\n",
        "\n",
        "    @classmethod\n",
        "    def setup(cls, cfg):\n",
        "        scaler = getattr(preprocessing, cfg.scaler)()\n",
        "        X = MNIST(cfg.data_dir, train=True, download=True).data.reshape(-1, 784).cpu().numpy()\n",
        "        Y = MNIST(cfg.data_dir, train=True, download=True).targets.cpu().numpy()\n",
        "        X = scaler.fit_transform(X)\n",
        "        X = X[:10000]\n",
        "        Y = Y[:10000]\n",
        "        return cls(X, Y)"
      ],
      "metadata": {
        "id": "AcoslAXNY6v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we manually copy it to the dataset.py file.\n"
      ],
      "metadata": {
        "id": "2_84w55XZGSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: run clustering training"
      ],
      "metadata": {
        "id": "HYw5RV9mZJRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "import numpy as np\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "import os\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "from train_evaluate import BaseModule\n",
        "\n",
        "\n",
        "cfg = OmegaConf.load(\"cfg/cfg_mnist.yaml\")\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "if not cfg.validate:\n",
        "    cfg.trainer.check_val_every_n_epoch = cfg.trainer.max_epochs + 1 # the validation will be never done\n",
        "with open(f\"results_example.txt\", mode='a') as f:\n",
        "    header = '\\t'.join(['seed', 'acc', 'ari', 'nmi', 'local_gates', 'global_gates',\n",
        "                        'topk_max_silhouette_score', 'topk_min_dbi_score'])\n",
        "    f.write(f\"{header}\\n\")\n",
        "\n",
        "\n",
        "for seed in range(cfg.seeds):\n",
        "    cfg.seed = seed\n",
        "    seed_everything(seed)\n",
        "    np.random.seed(seed)\n",
        "    if not os.path.exists(cfg.dataset):\n",
        "        os.makedirs(cfg.dataset)\n",
        "    model = BaseModule(cfg)\n",
        "    logger = TensorBoardLogger(cfg.dataset, name=\"example\", log_graph=False)\n",
        "    trainer = Trainer(**cfg.trainer, callbacks=[LearningRateMonitor(logging_interval='step')])\n",
        "    trainer.logger = logger\n",
        "    trainer.fit(model)\n",
        "    topk_max_siluetter_score = np.mean(sorted(model.max_silhouette_score, reverse=True)[:10])\n",
        "    topk_min_dbi_score = np.mean(sorted(model.max_silhouette_score)[:10])\n",
        "    results_str = '\\t'.join(\n",
        "        [f'{seed}',\n",
        "         f'{model.best_acc}',\n",
        "         f'{model.best_ari}',\n",
        "         f'{model.best_nmi}',\n",
        "         f'{model.best_local_feats}',\n",
        "         f'{model.best_global_feats}',\n",
        "         f'{topk_max_siluetter_score}',\n",
        "         f'{topk_min_dbi_score}',\n",
        "         ])\n",
        "    with open(f\"results_example.txt\", mode='a') as f:\n",
        "        f.write(f\"{results_str}\\n\")\n",
        "        f.flush()\n"
      ],
      "metadata": {
        "id": "2VO1jSYjZJzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: check you results output file\n",
        "\n",
        "We measure the next metrics:\n",
        "\n",
        "1. ACC\n",
        "2. ARI\n",
        "3. NMI\n",
        "4. BD-Index\n",
        "5. Silhouette Score"
      ],
      "metadata": {
        "id": "x57tV3yKZTYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVDADnHzZVHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}